{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンソールで設定したSparkとNoteBookを接続します(動かす前に毎度実行する必要があります)\n",
    "import findspark\n",
    "findspark.init(\"/home/pyspark/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# spark.xxxxxと記載することで処理を分散させることが可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 今回利用するデータの確認\n",
    "\n",
    "今回は、datafileフォルダ配下の\n",
    "\n",
    "- jinko.csv(各年代の都道府県ごとの人口)\n",
    "- kenmei_master.csv(都道府県コードをまとめている)\n",
    "\n",
    "を利用していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　データの読み込みを行う\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "struct = StructType([\n",
    "    StructField(\"code\", StringType(), False),\n",
    "    StructField(\"gengo\", StringType(), False),\n",
    "    StructField(\"wareki\", StringType(), False),\n",
    "    StructField(\"seireki\", StringType(), False),\n",
    "    StructField(\"chu\", StringType(), False),\n",
    "    StructField(\"sokei\", StringType(), False),\n",
    "    StructField(\"jinko_male\", StringType(), False),\n",
    "    StructField(\"jinko_female\", StringType(), False)\n",
    "])\n",
    "df=spark.read.option(\"multiLine\", \"true\").option(\"encoding\", \"UTF-8\") \\\n",
    "    .csv(\"./datafile/jinko.csv\", header=False, sep=',', inferSchema=False,schema=struct)\n",
    "\n",
    "struct2 = StructType([\n",
    "    StructField(\"code\", StringType(), False),\n",
    "    StructField(\"kenmei\", StringType(), False)\n",
    "])\n",
    "df2=spark.read.option(\"multiLine\", \"true\").option(\"encoding\", \"UTF-8\") \\\n",
    "    .csv(\"./datafile/kenmei_master.csv\", header=False, sep=',', inferSchema=False,schema=struct2)\n",
    "\n",
    "\n",
    "struct3 = StructType([\n",
    "    StructField(\"codes\", StringType(), False),\n",
    "    StructField(\"kenmei\", StringType(), False)\n",
    "])\n",
    "df3=spark.read.option(\"multiLine\", \"true\").option(\"encoding\", \"UTF-8\") \\\n",
    "    .csv(\"./datafile/kenmei_master.csv\", header=False, sep=',', inferSchema=False,schema=struct3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# それぞれのデータの確認をしてみましょう\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テストレベルの設定\n",
    "\n",
    "どの単位でテストを行うかを考えることをテストレベルを設定すると言います。\n",
    "\n",
    "- テーブル単位でのテスト\n",
    "- カラム単位でのテスト\n",
    "- テーブル間単位でのテスト\n",
    "\n",
    "の3種類存在します。\n",
    "\n",
    "例えば、1つ〜カラムに対して確認を行うのであれば、カラム単位のテスト(辞書テスト、if-thenテストなど)です。  \n",
    "テーブル単位でのテストは一つのテーブル単位でテストを行うことです（0件テストやタイムラインネスなど）  \n",
    "テーブル間単位でのテストは、複数テーブル間でのテストを行うことです（コンシステンシーなど）  \n",
    "\n",
    "\n",
    "# まずは自身でデータを理解して、定義を考えていきます。\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 現状把握のための便利関数\n",
    "最大（maximum）、最小（ minimum）、平均（ mean）、中央値（ median）、最頻値（mode）、分散（ variance）、標準偏差（ standard deviation）基本統計量を取得を紹介します。\n",
    "\n",
    "主にデータの傾向を掴むために利用されます。\n",
    "\n",
    "テストというより、現状把握の意味合いの方が高いかもしれません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストとして役に立つことはあまり多くありませんが、データをさっと確認する時に役に立ちます\n",
    "# 確認後データのテスト計画を立てていくことになります。\n",
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If-thenテスト\n",
    "もしAの値が1ならばBの値は2のようなテストを行うのがif-thenテストです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if-then\n",
    "# カラム\n",
    "df = df.withColumn(\"gengo_wareki_if_then_check\",\n",
    "    F.when(F.col(\"gengo\") == \"大正\", \n",
    "        ((F.col(\"wareki\").cast(\"integer\") > 0) & (F.col(\"wareki\").cast(\"integer\") <= 62)).cast(\"long\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ゼロコントロール\n",
    "四則演算の結果について確認するのが、ゼロコントロールです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ゼロコントロール\n",
    "# カラム\n",
    "df.withColumn(\n",
    "    'sokei_check_zero_control', \n",
    "    (F.col('sokei') == (F.col('jinko_male') + F.col('jinko_female'))).cast(\"long\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# レンジテストと辞書テスト\n",
    "データが特定の範囲に入っているのか？確認するのがレンジテストと辞書テストです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辞書テスト\n",
    "# カラム\n",
    "df.withColumn(\"gengo_dictionary_chek\", F.col(\"gengo\").isin(['大正','昭和','平成'])).show()\n",
    "\n",
    "# レンジテスト\n",
    "# カラム\n",
    "df.orderBy(F.col(\"seireki\").desc()).show()\n",
    "df.orderBy(F.col(\"seireki\").asc()).show()\n",
    "\n",
    "df.withColumn(\"seireki_range_check\", (F.col(\"seireki\").between(1920,2015)).cast(\"long\")).show()\n",
    "df.withColumn(\"seireki_range_check\", F.col(\"seireki\").between(1920,2015)).groupby(\"seireki_range_check\").count().show()\n",
    "\n",
    "#この時点ででた変なデータは後ほどリペアのレクチャーで除外します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nullチェックとユニークネス\n",
    "データにNullが含まれていたりユニークでないとデータが非常に扱いにくいです。\n",
    "\n",
    "Nullチェックとユニークネスを通して扱いにくいデータを見つけ出していきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ユニークネス、PK\n",
    "#　こちらはPKの確認\n",
    "df.select(F.countDistinct(\"code\",\"gengo\", \"wareki\")).show()\n",
    "\n",
    "# 割合を判定する\n",
    "# ユニークネス\n",
    "#全体に対してどれだけユニークか？\n",
    "#countdistinct / 全体のレコード\n",
    "df.withColumn(\"unique_ness_check\", \n",
    "    F.lit(df.agg(F.countDistinct(\"code\",\"gengo\", \"wareki\").alias(\"countdistinct\")).collect()[0][0]) / F.lit(df.count())).show()\n",
    "\n",
    "# ユニークではない！\n",
    "\n",
    "# どうもnullがある様子\n",
    ">>> df.groupby(\"kenmei\").count().show(n=60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パターンチェック\n",
    "特定の正規表現にデータが沿っているのか？をチェックするパターンチェックについて学びます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正規表現はJavaの正規表現き表です\n",
    "# パターンチェック\n",
    "# カラム\n",
    "df.withColumn(\"seireki_pattern_chek\", (F.col(\"seireki\").rlike(\"\\d{4}\")).cast(\"long\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# コンシステンシー\n",
    "テーブル間の紐付きの割合で見るのは、エクスターナルコンシステンシー\n",
    "\n",
    "エクスターナルコンシステンシー  \n",
    "joinできるの？というのは大きな問題である  \n",
    "データは複数組み合わせて価値を生み出すものなので単体では役に立たないことが多い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今回はdfのcodeとdf2のcodeがどれだけ紐つくかを確認する\n",
    "# テーブル間\n",
    "\n",
    "# A-Bをして残ったら一致していないものがある\n",
    "df.select(\"code\").subtract(df2.select(\"code\")).show()\n",
    "\n",
    "# 全体の件数と、一致数件数の割合をとってみる方式でもOK\n",
    "df.withColumn(\"code_consistency_check\", \n",
    "    F.lit(df.select(\"code\").distinct().count()) / F.lit(df.select(\"code\").intersect(df2.select(\"code\")).count())).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# レイショーコントロール\n",
    "\n",
    "想定した割合にデータの件数や統計量が収まっているかどうかをテストする方法。割合制御とも呼ばれる。  \n",
    "男女の出生率がおおよそ1：1であることを利用して集めたデータの男女比に、極端な差がないかの比を比較し確認することなども含まれる\n",
    "\n",
    "また、急にデータが増えたなどのチェックにも使われる。    \n",
    "例えばリリースの不具合で急激に件数が増えた！  \n",
    "なんてことにも利用できたりします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio_control\n",
    "# カラム(テーブルでも可能。例えばテーブルの件数など)\n",
    "df.withColumn(\"jinko_male_jinko_female_ratio_check\", \n",
    "    F.col(\"jinko_male\").cast(\"integer\") / F.col(\"jinko_female\").cast(\"integer\")) \\\n",
    "        .withColumn(\"ratio_check\", (F.col(\"jinko_male_jinko_female_ratio_check\").between(0.8, 1.2)).cast(\"long\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# タイムラインネス\n",
    "データがしっかりと特定の時間に処理されているか確認する方法です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# タイムラインネス\n",
    "# カラム\n",
    "# 少し運用ちっくですが、必ずETLなどで処理した時間をテーブルの末尾に追加しておくと良いです。\n",
    "# pythonであればosの機能を使ってファイルの更新時間を取得することができますが、分散基盤になると使いづらいのです。\n",
    "df.withColumn(\"timelineness_check\", F.current_timestamp()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メタデータの品質テスト\n",
    "\n",
    "メタデータの名寄せ  \n",
    "code で　一方が idだったらjoinをためらってしまいませんか？  \n",
    "事前に準備するというより、既にめちゃくちゃな状態でそれを修正するために探索していくことが多いです。  \n",
    "そのため、データのフォーマットから実は同じじゃない？というサジェストをしていくと良い  \n",
    "\n",
    "\n",
    "今回はdf2とdf3のチグハグについて考えてみようと思います  \n",
    "みると一目瞭然ですが、一方はcodes、もう一方はcodeになっています  \n",
    "PJとして2桁の数値はcodeという名称とした場合  \n",
    "そんな時に使えるのがエクスターナルコンシステンシーです.\n",
    "\n",
    "\n",
    "つまり一致数が高ければ、「あれ？これって同じ定義じゃないですか？」と言ったサジェストができることになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メタデータのサジェスト\n",
    "\n",
    "# コンシステンシーを確認\n",
    "# df2 <-> df3\n",
    "hoge = df.withColumn(\"master_data_consistency_check\", \n",
    "    F.lit(df2.select(\"code\").distinct().count()) / F.lit(df3.select(\"codes\").intersect(df2.select(\"code\")).count()))\n",
    "\n",
    "hoge.withColumn(\"code_metadata_suggest\", F.when(F.col(\"master_data_consistency_check\").cast(\"integer\") > 0.8, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0件チェック\n",
    "テーブル単位でのテスト。  \n",
    "急にデータが更新されていなかったりする際にすぐに気づくことができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0件チェック\n",
    "#　テーブル\n",
    "df.withColumn(\"count_check\", F.when(F.lit(df.count()) > 0, F.lit(1))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# カラム数チェック\n",
    "スキーマが急に変更されていないかをチェックする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今回は8カラムある\n",
    "#　テーブル\n",
    "df.withColumn(\"column_num_check\", F.when(F.lit(len(df.columns)) == 8,F.lit(1))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データのリペア\n",
    "\n",
    "データの不備を見つけたら、そのデータを修正したり削除したりする必要があります。\n",
    "\n",
    "データリペアとしては再集計を行う方法があります  \n",
    "再集計を行わずにできる方法もあり  \n",
    "Update文が打てる場合もたまにありますが、APIの上限があったりと使いやすものはあまりありません。  \n",
    "またDelete文がないので、削除するということはできません。\n",
    "\n",
    "そうなると結局再集計という道に落ち着くことが多いです。  \n",
    "\n",
    "今回は不要なデータを除いて再度利用するという再集計の方式を行ってみたいと思います。\n",
    "\n",
    "途中で見つかったいらないデータ\n",
    "\n",
    "```\n",
    "|都道府県コード| 元号|和暦（年）|西暦（年）|  注|人口（総数）|人口（男）|  人口（女）|            true|\n",
    "|1)　沖縄県は調査されなかったため...| null|  null|   null|null|    null|      null|        null|\n",
    "|2)　長野県西筑摩群山口村と岐阜県...| null|  null|   null|null|    null|      null|        null|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのリペア\n",
    "df.filter(F.col(\"code\")!=\"都道府県コード\") \\\n",
    "    .filter(~F.col(\"code\").contains(\"1)　沖縄県は調査され\"))\\\n",
    "    .filter(~F.col(\"code\").contains(\"2)　長野県西筑摩群山口村\")).show()\n",
    "\n",
    "# 軽く確認\n",
    "df.count()\n",
    "df.filter(F.col(\"code\")!=\"都道府県コード\") \\\n",
    "    .filter(~F.col(\"code\").contains(\"1)　沖縄県は調査され\"))\\\n",
    "    .filter(~F.col(\"code\").contains(\"2)　長野県西筑摩群山口村\")).count()\n",
    "\n",
    "rep_df = df.filter(F.col(\"code\")!=\"都道府県コード\") \\\n",
    "    .filter(~F.col(\"code\").contains(\"1)　沖縄県は調査され\"))\\\n",
    "    .filter(~F.col(\"code\").contains(\"2)　長野県西筑摩群山口村\"))\n",
    "\n",
    "rep_df.select(F.countDistinct(\"code\",\"gengo\", \"wareki\")/ rep_df.count()).show()\n",
    "\n",
    "rep_df.withColumn(\"unique_ness_check\", \n",
    "    F.lit(rep_df.agg(F.countDistinct(\"code\",\"gengo\", \"wareki\").alias(\"countdistinct\")).collect()[0][0]) / F.lit(rep_df.count())).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ストリーミングのテスト\n",
    "ストリーミングのテストは、ストリーミングで流れている際に行われることはあまりありません。\n",
    "\n",
    "その代わりストリーミングではテストが不要になるようにAvroというフォーマットでデータの担保されることが多いです。\n",
    "\n",
    "## Avro フォーマットとは？\n",
    "\n",
    "もう一つがHadoopの生みの親であるDoug Cutting氏によりプロジェクト化されたAvro（アブロ）フォーマットです【URL】https://avro.apache.org。  \n",
    "Avroフォーマットはおもにストリーミングでのやり取りで効力を発揮するフォーマットです。 \n",
    "\n",
    "元々AvroはHadoopの弱点であったJavaでしか読み書きできないという言語のポータビリティを解決するために生まれました言語の  \n",
    "ポータビリティーが低いということはそのままAvroファイルと連携する対向のシステムの利用言語まで縛ってしまう可能性があります。 　\n",
    "\n",
    "Avroフォーマットには、送信側に対する型や入力値を規定ことができます。\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "     \"type\": \"record\",\n",
    "     \"namespace\": \"com.example\",\n",
    "     \"name\": \"FullName\",\n",
    "     \"fields\": [\n",
    "       { \"name\": \"first\", \"type\": \"string\" },\n",
    "       { \"name\": \"last\", \"type\": \"string\" }\n",
    "     ]\n",
    "} \n",
    "```\n",
    "\n",
    "# じゃあテストしない？\n",
    "それではテストしないか？というわけではなくて、ストリーミングもデータが溜まってくると結局データとしてはバッチデータとなります。\n",
    "\n",
    "そのため、最終的には今まで見てきたようなテスト方法を適用することとなります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最後はSparkをクローズする\n",
    "spark.stop()\n",
    "spark.sparkContext.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
