{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンソールで設定したSparkとNoteBookを接続します(動かす前に毎度実行する必要があります)\n",
    "import findspark\n",
    "findspark.init(\"/home/pyspark/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pysparkに必要なライブラリを読み込む\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark sessionの作成\n",
    "# spark.ui.enabled trueとするとSparkのGUI画面を確認することができます\n",
    "# spark.eventLog.enabled true　とすると　GUIで実行ログを確認することができます\n",
    "# GUIなどの確認は最後のセクションで説明を行います。\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"chapter1\") \\\n",
    "    .config(\"hive.exec.dynamic.partition\", \"true\") \\\n",
    "    .config(\"hive.exec.dynamic.partition.mode\", \"nonstrict\") \\\n",
    "    .config(\"spark.sql.session.timeZone\", \"JST\") \\\n",
    "    .config(\"spark.ui.enabled\",\"true\") \\\n",
    "    .config(\"spark.eventLog.enabled\",\"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# spark.xxxxxと記載することで処理を分散させることが可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 今回利用するデータの確認\n",
    "\n",
    "今回は、datafileフォルダ配下の\n",
    "\n",
    "- jinko.csv(各年代の都道府県ごとの人口)\n",
    "- kenmei_master.csv(都道府県コードをまとめている)\n",
    "\n",
    "を利用していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　データの読み込みを行う\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "struct = StructType([\n",
    "    StructField(\"code\", StringType(), False),\n",
    "    StructField(\"gengo\", StringType(), False),\n",
    "    StructField(\"wareki\", StringType(), False),\n",
    "    StructField(\"seireki\", StringType(), False),\n",
    "    StructField(\"chu\", StringType(), False),\n",
    "    StructField(\"sokei\", StringType(), False),\n",
    "    StructField(\"jinko_male\", StringType(), False),\n",
    "    StructField(\"jinko_female\", StringType(), False)\n",
    "])\n",
    "df=spark.read.option(\"multiLine\", \"true\").option(\"encoding\", \"UTF-8\") \\\n",
    "    .csv(\"./datafile/jinko.csv\", header=False, sep=',', inferSchema=False,schema=struct)\n",
    "\n",
    "struct2 = StructType([\n",
    "    StructField(\"code\", StringType(), False),\n",
    "    StructField(\"kenmei\", StringType(), False)\n",
    "])\n",
    "df2=spark.read.option(\"multiLine\", \"true\").option(\"encoding\", \"UTF-8\") \\\n",
    "    .csv(\"./datafile/kenmei_master.csv\", header=False, sep=',', inferSchema=False,schema=struct2)\n",
    "\n",
    "\n",
    "struct3 = StructType([\n",
    "    StructField(\"codes\", StringType(), False),\n",
    "    StructField(\"kenmei\", StringType(), False)\n",
    "])\n",
    "df3=spark.read.option(\"multiLine\", \"true\").option(\"encoding\", \"UTF-8\") \\\n",
    "    .csv(\"./datafile/kenmei_master.csv\", header=False, sep=',', inferSchema=False,schema=struct3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# それぞれのデータの確認をしてみましょう\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テストレベルの設定\n",
    "\n",
    "どの単位でテストを行うかを考えることをテストレベルを設定すると言います。\n",
    "\n",
    "- テーブル単位でのテスト\n",
    "- カラム単位でのテスト\n",
    "- テーブル間単位でのテスト\n",
    "\n",
    "の3種類存在します。\n",
    "\n",
    "例えば、1つ〜カラムに対して確認を行うのであれば、カラム単位のテスト(辞書テスト、if-thenテストなど)です。  \n",
    "テーブル単位でのテストは一つのテーブル単位でテストを行うことです（0件テストやタイムラインネスなど）  \n",
    "テーブル間単位でのテストは、複数テーブル間でのテストを行うことです（コンシステンシーなど）  \n",
    "\n",
    "\n",
    "# まずは自身でデータを理解して、定義を考えていきます。\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 現状把握のための便利関数\n",
    "最大（maximum）、最小（ minimum）、平均（ mean）、中央値（ median）、最頻値（mode）、分散（ variance）、標準偏差（ standard deviation）基本統計量を取得を紹介します。\n",
    "\n",
    "主にデータの傾向を掴むために利用されます。\n",
    "\n",
    "テストというより、現状把握の意味合いの方が高いかもしれません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストとして役に立つことはあまり多くありませんが、データをさっと確認する時に役に立ちます\n",
    "# 確認後データのテスト計画を立てていくことになります。\n",
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If-thenテスト\n",
    "もしAの値が1ならばBの値は2のようなテストを行うのがif-thenテストです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if-then\n",
    "df.withColumn(\"gengo_wareki_if_then\",\n",
    "    F.when(F.col(\"gengo\") == \"昭和\", \n",
    "        (F.col(\"wareki\").cast(\"integer\") > 0) & (F.col(\"wareki\").cast(\"integer\") <= 62)\n",
    "    )\n",
    ").filter(F.col(\"gengo\") == \"昭和\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ゼロコントロール\n",
    "四則演算の結果について確認するのが、ゼロコントロールです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ゼロコントロール\n",
    "df.withColumn(\n",
    "    'sokei_check_zero_control', \n",
    "    F.col('sokei') == (F.col('jinko_male') + F.col('jinko_female'))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# レンジテストと辞書テスト\n",
    "データが特定の範囲に入っているのか？確認するのがレンジテストと辞書テストです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辞書テスト\n",
    "df.withColumn(\"gengo_dictionary_chek\", F.col(\"gengo\").isin(['大正','昭和','平成'])).show()\n",
    "\n",
    "# レンジテスト\n",
    "df.orderBy(F.col(\"seireki\").desc()).show()\n",
    "df.orderBy(F.col(\"seireki\").asc()).show()\n",
    "\n",
    "df.withColumn(\"seireki_range_check\", F.col(\"seireki\").between(1920,2015)).show()\n",
    "df.withColumn(\"seireki_range_check\", F.col(\"seireki\").between(1920,2015)).groupby(\"seireki_range_check\").count().show()\n",
    "\n",
    "#この時点ででた変なデータは後ほど除外します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nullチェックとユニークネス\n",
    "データにNullが含まれていたりユニークでないとデータが非常に扱いにくいです。\n",
    "\n",
    "Nullチェックとユニークネスを通して扱いにくいデータを見つけ出していきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ユニークネス、PK\n",
    "#　こちらはPK\n",
    ">>> df.select(F.countDistinct(\"code\",\"gengo\", \"wareki\")).show()\n",
    "+-----------------------------------+\n",
    "|count(DISTINCT code, gengo, wareki)|\n",
    "+-----------------------------------+\n",
    "|                                981|\n",
    "+-----------------------------------+\n",
    "\n",
    "# 割合を判定する\n",
    "# ユニークネス\n",
    "全体に対してどれだけユニークか？\n",
    "countdistinct / 全体のレコード\n",
    "\n",
    "\n",
    ">>> df.select(F.countDistinct(\"code\",\"gengo\", \"wareki\")/ df.count()).show()\n",
    "+-------------------------------------------+\n",
    "|(count(DISTINCT code, gengo, wareki) / 983)|\n",
    "+-------------------------------------------+\n",
    "|                         0.9979654120040692|\n",
    "+-------------------------------------------+\n",
    "\n",
    "# ユニークではない！\n",
    "\n",
    ">>> df.groupby(\"kenmei\").count()\n",
    "DataFrame[kenmei: string, count: bigint]\n",
    "# どうもnullがある様子\n",
    ">>> df.groupby(\"kenmei\").count().show(n=60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パターンチェック\n",
    "特定の正規表現にデータが沿っているのか？をチェックするパターンチェックについて学びます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正規表現はJavaの正規表現き表です\n",
    "# パターンチェック\n",
    "df.withColumn(\"seireki_pattern_chek\", F.col(\"seireki\").rlike(\"\\d{4}\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# コンシステンシー\n",
    "テーブル間の紐付きの割合で見るのは、エクスターナルコンシステンシー\n",
    "\n",
    "エクスターナルコンシステンシー  \n",
    "joinできるの？というのは大きな問題である  \n",
    "データは複数組み合わせて価値を生み出すものなので単体では役に立たないことが多い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今回はdfのcodeとdf2のcodeがどれだけ紐つくかを確認する\n",
    "\n",
    "df.select(\"code\").subtract(df2.select(\"code\")).show()\n",
    "\n",
    "df.select(\"code\").distinct().count()\n",
    "df.select(\"code\").intersect(df2.select(\"code\")).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# レイショーコントロール\n",
    "\n",
    "想定した割合にデータの件数や統計量が収まっているかどうかをテストする方法。割合制御とも呼ばれる。  \n",
    "男女の出生率がおおよそ1：1であることを利用して集めたデータの男女比に、極端な差がないかの比を比較し確認することなども含まれる\n",
    "\n",
    "また、急にデータが増えたなどのチェックにも使われる。    \n",
    "例えばリリースの不具合で急激に件数が増えた！  \n",
    "なんてことにも利用できたりします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio_control\n",
    "df.withColumn(\"ratio_check\", \n",
    "F.col(\"jinko_male\").cast(\"integer\") / F.col(\"jinko_female\").cast(\"integer\")) \\\n",
    "    .withColumn(\"ratio_check\",F.col(\"ratio_check\").between(0.8, 1.2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# タイムラインネス\n",
    "データがしっかりと特定の時間に処理されているか確認する方法です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# タイムラインネス\n",
    "# 少し運用ちっくですが、必ずETLなどで処理した時間をテーブルの末尾に追加しておくと良いです。\n",
    "# pythonであればosの機能を使ってファイルの更新時間を取得することができますが、分散基盤になると使いづらいのです。\n",
    "df.withColumn(\"timelineness_check\", F.current_timestamp()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メタデータの品質テスト\n",
    "\n",
    "メタデータの名寄せ  \n",
    "code で　一方が idだったらjoinをためらってしまいませんか？  \n",
    "事前に準備するというより、既にめちゃくちゃな状態でそれを修正するために探索していくことが多いです。  \n",
    "そのため、データのフォーマットから実は同じじゃない？というサジェストをしていくと良い  \n",
    "今回はdf2とdf3のチグハグについて考えてみようと思います  \n",
    "みると一目瞭然ですが、一方はcodes、もう一方はcodeになっています  \n",
    "PJとして2桁の数値はcodeという名称とした場合  \n",
    "そんな時に使えるのがエクスターナルコンシステンシーです"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0件チェック\n",
    "テーブル単位でのテスト。  \n",
    "急にデータが更新されていなかったりする際にすぐに気づくことができる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0件チェック\n",
    "df.withColumn(\"count_check\", F.when(F.lit(df.count()) > 0,True)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# カラム数チェック\n",
    "スキーマが急に変更されていないかをチェックする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今回は8カラムある\n",
    "df.withColumn(\"column_num_check\", F.when(F.lit(len(df.columns)) = 8,True)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データのリペア\n",
    "データリペアとしては再集計を行う方法があります  \n",
    "再集計を行わずにできる方法もあり  \n",
    "Update文が打てる場合もたまにありますが、APIの上限があったりと使いやすものはあまりありません。  \n",
    "そうなると結局再集計という道に落ち着くことが多いです。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのリペア\n",
    "このデータを除外する例を出してみる\n",
    "|都道府県コード| 元号|和暦（年）|西暦（年）|  注|人口（総数）|人口（男）|  人口（女）|            true|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ストリーミングのテスト\n",
    "ストリーミングのテストはAvroで担保されることが多いということを書く"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最後はSparkをクローズする\n",
    "spark.stop()\n",
    "spark.sparkContext.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cadefa720d1a2267f4d12d08d812560a64cfe891877bc388bf0e8af3e4846067"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
